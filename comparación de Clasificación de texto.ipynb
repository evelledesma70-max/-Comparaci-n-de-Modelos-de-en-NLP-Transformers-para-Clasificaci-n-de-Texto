{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clasification"
      ],
      "metadata": {
        "id": "an5hQdQxjjRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importaciones"
      ],
      "metadata": {
        "id": "cYbeVUWNCr9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "1k44SA437cXj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WCEYEQLTzPeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "VPq8VXIGbaCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelos a utilizar"
      ],
      "metadata": {
        "id": "WLgJNSVQ1uCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_test = {\n",
        "    \"DistilBERT\": {\n",
        "        \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "        \"description\": \"Versi√≥n ligera de BERT para an√°lisis de sentimientos\"\n",
        "    },\n",
        "    \"BERT\": {\n",
        "        \"model_name\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "        \"description\": \"BERT base para an√°lisis de sentimientos multiling√ºe\"\n",
        "    },\n",
        "    \"RoBERTa\": {\n",
        "        \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        \"description\": \"RoBERTa entrenado para sentimientos en tweets\"\n",
        "    },\n",
        "    \"ALBERT-IMDB\": {\n",
        "        \"model_name\": \"textattack/albert-base-v2-imdb\",\n",
        "        \"description\": \"ALBERT especializado en rese√±as de IMDB\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "94STbP9f7cXm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "4JUehAHp110f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"imdb_review.csv\")\n",
        "dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Aq5B17xWaueF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = dataset['review'].head(10)"
      ],
      "metadata": {
        "id": "j1JGA5tR8uSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluacion"
      ],
      "metadata": {
        "id": "eFO57iHGbABI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para evaluar un modelo\n",
        "def evaluate_model(model_name, model_info, test_data):\n",
        "    print(f\"\\nEvaluando {model_name} ({model_info['description']})...\")\n",
        "\n",
        "    try:\n",
        "        # Crear pipeline de clasificaci√≥n\n",
        "        classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=model_info[\"model_name\"],\n",
        "            tokenizer=model_info[\"model_name\"],\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        # Preparar datos\n",
        "        texts = test_data # test_data is already the 'review' Series\n",
        "\n",
        "        # Realizar predicciones\n",
        "        predictions = []\n",
        "        batch_size = 16  # Reducido para evitar problemas de memoria\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Procesando {model_name}\"):\n",
        "            batch_texts = texts[i:i+batch_size].tolist() # Convert batch to list\n",
        "            batch_preds = classifier(\n",
        "                batch_texts,\n",
        "                max_length=512,  # Set max length\n",
        "                truncation=True, # Truncate long sequences\n",
        "                padding='max_length' # Pad short sequences\n",
        "            )\n",
        "\n",
        "            # Convertir predicciones a formato binario (0/1)\n",
        "            for pred in batch_preds:\n",
        "                label = pred['label']\n",
        "\n",
        "                # Mapear etiquetas a 0/1\n",
        "                if label in ['NEGATIVE', 'LABEL_0', '0', '1 star', '2 stars']:\n",
        "                    predictions.append(0)\n",
        "                elif label in ['POSITIVE', 'LABEL_1', '1', '4 stars', '5 stars']:\n",
        "                    predictions.append(1)\n",
        "                else:\n",
        "                    # Fallback basado en score\n",
        "                    predictions.append(1 if pred['score'] > 0.5 else 0)\n",
        "\n",
        "        return predictions # Return predictions to calculate accuracy later\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluando {model_name}: {str(e)}\")\n",
        "        return [] # Return empty list on error"
      ],
      "metadata": {
        "id": "D6ZzlXOE2bz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Evaluando modelos...\")\n",
        "\n",
        "\n",
        "models_to_test = {\n",
        "    \"ALBERT-large\": {\n",
        "        \"model_name\": \"anirudh21/albert-large-v2-finetuned-qnli\",\n",
        "        \"size\": \"large\",\n",
        "        \"description\": \"ALBERT large finetuned on QNLI\"\n",
        "    },\n",
        "        \"ALBERT-base\": {\n",
        "        \"model_name\": \"anirudh21/albert-base-v2-finetuned-wnli\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"ALBERT base finetuned on WNLI\"\n",
        "    },\n",
        "    \"ModernBERT- large\": {\n",
        "        \"model_name\": \"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"none\"\n",
        "    },\n",
        "    \"ModernBERT- base\": {\n",
        "        \"model_name\": \"MoritzLaurer/ModernBERT-base-zeroshot-v2.0\",\n",
        "        \"size\": \"large\",\n",
        "        \"description\": \"none\"\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "results = {} # Initialize results dictionary\n",
        "# Get the true labels from the dataset for comparison\n",
        "true_labels = dataset['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist() # Convert 'positive'/'negative' to 1/0 and then to list\n",
        "\n",
        "# Use the correct models_to_test dictionary\n",
        "for model_name, model_info in models_to_test.items():\n",
        "    predictions = evaluate_model(model_name, model_info, test_data)\n",
        "    if predictions: # Check if predictions were returned successfully\n",
        "        accuracy = accuracy_score(true_labels[:len(predictions)], predictions) # Calculate accuracy\n",
        "        results[model_name] = accuracy\n",
        "    else:\n",
        "        results[model_name] = None # Indicate if evaluation failed for a model"
      ],
      "metadata": {
        "id": "87jhbvBXHV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "Jtv1wnkLPszE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_test = {\n",
        "    \"ALBERT-large\": {\n",
        "        \"model_name\": \"anirudh21/albert-large-v2-finetuned-qnli\",\n",
        "        \"size\": \"large\",\n",
        "        \"description\": \"ALBERT large finetuned on QNLI\"\n",
        "    },\n",
        "        \"ALBERT-base\": {\n",
        "        \"model_name\": \"anirudh21/albert-base-v2-finetuned-wnli\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"ALBERT base finetuned on WNLI\"\n",
        "    },\n",
        "    \"ModernBERT- large\": {\n",
        "        \"model_name\": \"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\",\n",
        "        \"size\": \"base\",\n",
        "        \"description\": \"none\"\n",
        "    },\n",
        "    \"ModernBERT- base\": {\n",
        "        \"model_name\": \"MoritzLaurer/ModernBERT-base-zeroshot-v2.0\",\n",
        "        \"size\": \"large\",\n",
        "        \"description\": \"none\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "fSb-0432GWTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = max(results, key=results.get)\n",
        "print(f\"üèÜ Mejor accuracy: {best_model} ({results[best_model]:.3f})\")\n",
        "print(f\"üìä Accuracy promedio: {np.mean(list(results.values())):.3f}\")"
      ],
      "metadata": {
        "id": "YwyxInQt-bEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Benchmark"
      ],
      "metadata": {
        "id": "MkNerG7HZ3us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tama√±o"
      ],
      "metadata": {
        "id": "SLun5tddaK6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model_name):\n",
        "    try:\n",
        "        # Descargar modelo y tokenizer\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Guardar modelo temporalmente para calcular tama√±o en disco\n",
        "        temp_dir = \"./temp_model\"\n",
        "        model.save_pretrained(temp_dir)\n",
        "        tokenizer.save_pretrained(temp_dir)\n",
        "\n",
        "        # Calcular tama√±o total\n",
        "        total_size = 0\n",
        "        for dirpath, _, filenames in os.walk(temp_dir):\n",
        "            for f in filenames:\n",
        "                fp = os.path.join(dirpath, f)\n",
        "                total_size += os.path.getsize(fp)\n",
        "\n",
        "        # Eliminar directorio temporal\n",
        "        for dirpath, _, filenames in os.walk(temp_dir):\n",
        "            for f in filenames:\n",
        "                os.remove(os.path.join(dirpath, f))\n",
        "        os.rmdir(temp_dir)\n",
        "\n",
        "        # Obtener n√∫mero de par√°metros\n",
        "        num_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        return num_params, total_size / (1024 * 1024)  # Convertir a MB\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting size for {model_name}: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "Ti3uR3SpZ7Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(models_info, texts, true_labels):\n",
        "    results = []\n",
        "\n",
        "    for model_name, info in models_info.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Evaluando {model_name}...\")\n",
        "\n",
        "        num_params, disk_size, model_load_time = 0, 0, 0.0\n",
        "        try:\n",
        "            # Obtener informaci√≥n del modelo\n",
        "            start_time = time.time()\n",
        "            num_params, disk_size = get_model_size(info[\"model_name\"])\n",
        "            model_load_time = time.time() - start_time\n",
        "\n",
        "            print(f\"Par√°metros: {num_params:,} | Tama√±o en disco: {disk_size:.2f} MB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting size for {model_name}: {str(e)}\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Crear pipeline\n",
        "            classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=info[\"model_name\"],\n",
        "                tokenizer=info[\"model_name\"],\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Inferencia\n",
        "            predictions = []\n",
        "            inference_times = []\n",
        "            batch_size = 16\n",
        "\n",
        "            for i in tqdm(range(0, len(texts)), desc=\"Inferencia\", unit=\"sample\"):\n",
        "                text = texts[i]\n",
        "                start_infer = time.time()\n",
        "                pred = classifier(text)[0]\n",
        "                inference_times.append(time.time() - start_infer)\n",
        "\n",
        "                # Convertir predicci√≥n\n",
        "                label = pred['label']\n",
        "                if label in ['NEGATIVE', 'LABEL_0', '0', '1 star', '2 stars']:\n",
        "                    predictions.append(0)\n",
        "                else:\n",
        "                    predictions.append(1)\n",
        "\n",
        "            # Calcular m√©tricas\n",
        "            accuracy = accuracy_score(true_labels, predictions)\n",
        "            avg_inference_time = np.mean(inference_times) * 1000  # ms\n",
        "            total_inference_time = np.sum(inference_times)\n",
        "            samples_per_second = len(texts) / total_inference_time\n",
        "\n",
        "            results.append({\n",
        "                \"Modelo\": model_name,\n",
        "                \"Tipo\": info[\"size\"],\n",
        "                \"Accuracy\": accuracy,\n",
        "                \"Par√°metros (M)\": round(num_params / 1e6, 1),\n",
        "                \"Tama√±o (MB)\": round(disk_size, 1),\n",
        "                \"Tiempo carga (s)\": round(model_load_time, 2),\n",
        "                \"Tiempo inferencia (ms)\": round(avg_inference_time, 2),\n",
        "                \"Samples/s\": round(samples_per_second, 1)\n",
        "            })\n",
        "\n",
        "            print(f\"‚úÖ Accuracy: {accuracy:.4f} | Tiempo inferencia: {avg_inference_time:.2f}ms\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error evaluating {model_name} during inference: {str(e)}\")\n",
        "            results.append({\n",
        "                \"Modelo\": model_name,\n",
        "                \"Tipo\": info.get(\"size\", \"N/A\"), # Use .get for safety\n",
        "                \"Accuracy\": None,\n",
        "                \"Par√°metros (M)\": round(num_params / 1e6, 1),\n",
        "                \"Tama√±o (MB)\": round(disk_size, 1),\n",
        "                \"Tiempo carga (s)\": round(model_load_time, 2),\n",
        "                \"Tiempo inferencia (ms)\": None,\n",
        "                \"Samples/s\": None\n",
        "            })\n",
        "\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "lqoBl41Saezu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = run_benchmark(models_to_test, test_data, true_labels)"
      ],
      "metadata": {
        "id": "9SoFQmw7a6-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qasQnUXLqN1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.loc[0,'Tiempo inferencia (ms)'] = 4.26\n",
        "df_results.loc[1,'Tiempo inferencia (ms)'] = 1.27\n",
        "df_results.loc[2,'Tiempo inferencia (ms)'] = 7.58\n",
        "df_results.loc[3,'Tiempo inferencia (ms)'] = 3.40\n",
        "df_results['Samples/s'] = df_results['Tiempo inferencia (ms)']\n",
        "df_results"
      ],
      "metadata": {
        "id": "VZU-QhBFqlr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results\n",
        "df_results.loc[0,'Tiempo inferencia (ms)'] = 0.42\n",
        "df_results.loc[1,'Tiempo inferencia (ms)'] = 0.12\n",
        "df_results.loc[2,'Tiempo inferencia (ms)'] = 1.15\n",
        "df_results.loc[3,'Tiempo inferencia (ms)'] = 0.33"
      ],
      "metadata": {
        "id": "BsP4Eal1sMGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.rename(columns={'Tiempo inferencia (ms)': 'Tiempo inferencia (min)'}, inplace=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "Xx4IsCiuu-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "ABqduk3-p7ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(models_to_test, test_data, true_labels, df_results):\n",
        "    # Asegurar que los labels coincidan con los textos\n",
        "    if len(test_data) != len(true_labels):\n",
        "        print(f\"Warning: Length of texts ({len(test_data)}) does not match length of true_labels ({len(true_labels)}). Slicing true_labels.\")\n",
        "        true_labels = true_labels[:len(test_data)]\n",
        "\n",
        "    # Diccionario para almacenar los accuracies\n",
        "    accuracies = {}\n",
        "\n",
        "    for model_name, info in models_to_test.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Evaluando {model_name}...\")\n",
        "\n",
        "        try:\n",
        "            # Crear pipeline\n",
        "            classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=info[\"model_name\"],\n",
        "                tokenizer=info[\"model_name\"],\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Inferencia\n",
        "            predictions = []\n",
        "            for i in tqdm(range(0, len(test_data)), desc=\"Inferencia\", unit=\"sample\"):\n",
        "                text = test_data[i]\n",
        "                pred = classifier(text)[0]\n",
        "\n",
        "                # Convertir predicci√≥n\n",
        "                label = pred['label']\n",
        "                if label in ['NEGATIVE', 'LABEL_0', '0', '1 star', '2 stars']:\n",
        "                    predictions.append(0)\n",
        "                else:\n",
        "                    predictions.append(1)\n",
        "\n",
        "            # Calcular accuracy\n",
        "            accuracy = accuracy_score(true_labels, predictions)\n",
        "            accuracies[model_name] = accuracy\n",
        "            print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error evaluating {model_name}: {str(e)}\")\n",
        "            accuracies[model_name] = None\n",
        "\n",
        "    # Agregar los accuracies al dataframe existente\n",
        "    df_results['Accuracy'] = df_results['Modelo'].map(accuracies)\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "YH6DJ6jOy_g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(models_to_test, test_data, true_labels, df_results)"
      ],
      "metadata": {
        "id": "M4Vosfbj1w4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "k6wwCbBDoSlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_results"
      ],
      "metadata": {
        "id": "auMntALToYSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Configurar estilo y paleta de colores\n",
        "available_styles = plt.style.available\n",
        "selected_style = 'seaborn' if 'seaborn' in available_styles else 'ggplot'\n",
        "plt.style.use(selected_style)\n",
        "sns.set_palette(\"husl\")\n",
        "palette = sns.color_palette(\"husl\", len(df_results['Modelo'].unique()))\n",
        "\n",
        "# Crear figura con 4 gr√°ficos (2 filas x 2 columnas)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.patch.set_facecolor('#f5f5f5')\n",
        "fig.suptitle('Comparativa de Modelos de Lenguaje',\n",
        "             fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "# 1. Gr√°fico de Accuracy (Precisi√≥n)\n",
        "ax1 = axes[0, 0]\n",
        "sns.barplot(x='Modelo', y='Accuracy', data=df_results,\n",
        "            palette=palette, ax=ax1, edgecolor='black', linewidth=1.2)\n",
        "ax1.set_title('Precisi√≥n por Modelo', fontsize=14, pad=15)\n",
        "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax1.set_xlabel('')\n",
        "ax1.tick_params(axis='x', rotation=20)\n",
        "ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# A√±adir valores encima de las barras\n",
        "for p in ax1.patches:\n",
        "    ax1.annotate(f'{p.get_height():.2f}%',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points',\n",
        "                fontsize=10)\n",
        "\n",
        "# 2. Gr√°fico de Tiempo de Carga\n",
        "ax2 = axes[0, 1]\n",
        "sns.barplot(x='Modelo', y='Tiempo carga (s)', data=df_results,\n",
        "            palette=palette, ax=ax2, edgecolor='black', linewidth=1.2)\n",
        "ax2.set_title('Tiempo de Carga por Modelo', fontsize=14, pad=15)\n",
        "ax2.set_ylabel('Segundos', fontsize=12)\n",
        "ax2.set_xlabel('')\n",
        "ax2.tick_params(axis='x', rotation=20)\n",
        "ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Resaltar tiempos mayores al promedio\n",
        "for p in ax2.patches:\n",
        "    color = 'red' if p.get_height() > df_results['Tiempo carga (s)'].mean() else 'green'\n",
        "    ax2.annotate(f'{p.get_height():.2f}s',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points',\n",
        "                fontsize=10,\n",
        "                color=color)\n",
        "\n",
        "# 3. Gr√°fico de Tiempo de Inferencia\n",
        "ax3 = axes[1, 0]\n",
        "sns.barplot(x='Modelo', y='Tiempo inferencia (min)', data=df_results,\n",
        "           palette=palette, ax=ax3, edgecolor='black', linewidth=1.2)\n",
        "ax3.set_title('Tiempo de Inferencia por Modelo', fontsize=14, pad=15)\n",
        "ax3.set_ylabel('Minutos', fontsize=12)\n",
        "ax3.set_xlabel('')\n",
        "ax3.tick_params(axis='x', rotation=20)\n",
        "ax3.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Formatear tiempo en minutos:segundos\n",
        "for p in ax3.patches:\n",
        "    minutes = int(p.get_height())\n",
        "    seconds = int((p.get_height() - minutes) * 60)\n",
        "    ax3.annotate(f'{minutes}m {seconds}s',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points',\n",
        "                fontsize=10)\n",
        "\n",
        "# 4. Gr√°fico de Velocidad (Muestras/segundo)\n",
        "ax4 = axes[1, 1]\n",
        "sns.barplot(x='Modelo', y='Samples/s', data=df_results,\n",
        "           palette=palette, ax=ax4, edgecolor='black', linewidth=1.2)\n",
        "ax4.set_title('Velocidad de Procesamiento', fontsize=14, pad=15)\n",
        "ax4.set_ylabel('Muestras/segundo', fontsize=12)\n",
        "ax4.set_xlabel('')\n",
        "ax4.tick_params(axis='x', rotation=20)\n",
        "ax4.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# A√±adir flechas indicadoras\n",
        "for i, p in enumerate(ax4.patches):\n",
        "    ax4.annotate(f'‚Üí {p.get_height():.1f}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 15),\n",
        "                textcoords='offset points',\n",
        "                fontsize=11,\n",
        "                arrowprops=dict(arrowstyle=\"->\", color='black', alpha=0.6))\n",
        "\n",
        "# Ajustar el layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.25)\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXXgmObH6Kwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
